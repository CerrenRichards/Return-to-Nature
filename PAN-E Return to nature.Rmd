---
title: "PAN-E Return to nature"
author: "Cerren Richards"
date: "14/05/2022"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here we use the Google mobility data to explore the response of people returning to nature after COVID-19 confinements. 

#_______________________

### Google Data

**Data Description**:

* How visits and length of stay at different places change compared to a baseline.
    - Parks: national parks, public beaches, marinas, dog parks, plazas, and public gardens.
    - Residential: places of residence
    - Grocery & Pharmacy: grocery markets, food warehouses, farmers markets, 
                          specialty food shops, drug stores, and pharmacies.
    - Retail & Recreation:restaurants, cafes, shopping centers, theme parks,
                          museums, libraries, and movie theaters
    
* Baseline: Median value, for the corresponding day of the week, during the 5- week period Jan 3–Feb 6, 2020

**Download data and methods**:

* https://www.google.com/covid19/mobility/
* https://support.google.com/covid19-mobility/answer/9824897?hl=en&ref_topic=9822927
* https://www.google.com/covid19/mobility/data_documentation.html?hl=en


#_______________________

## Download the Google mobility data

In this chunk we download the Google mobility data, reorganise it and select only the overall country trends. 

```{r, error=FALSE, warning=FALSE}

# These data were downloaded in September 2022
google <- read.csv("Global_Mobility_Report_latest.csv")

library(dplyr)
# remove the subregions and metro areas and only keep the overall country pattern
google <- google %>% filter(metro_area == "" & sub_region_1 == "")


library(naniar)
# R reads Namibia's iso2 code is as an "is.na" object, so we will rename it to NA
google$country_region_code[is.na(google$country_region_code)] <-"NA"

library(countrycode)

# Add the country codes so we ensure everything matches 
google$ISO2 <- google$country_region_code
google$ISO3 <- countrycode(google$ISO2, origin = 'iso2c', destination = 'iso3c')
google$country <- countrycode(google$ISO2, origin = 'iso2c', destination = 'country.name')

# Set the date as POSIXct
google$Date <- as.POSIXct(google$date, format='%Y-%m-%d')


# Select the specific data and rename columns
google_countries <- google %>% select(country, ISO3, Date, 
                    `Parks & Beaches`= 
                     parks_percent_change_from_baseline,
                    `Grocery & Pharmacy` = 
                     grocery_and_pharmacy_percent_change_from_baseline,
                     Residential = 
                     residential_percent_change_from_baseline,
                    `Retail & Recreation` = 
                      retail_and_recreation_percent_change_from_baseline) 

```

### Remove countries

The following 12 countries will be excluded because they do not show obvious change in lockdown dates or they do not have enough data (Bates et al. (2021) Biological Conservation):

"Hong Kong SAR China", "Mongolia", "South Korea", "Taiwan", "Yemen", "Liechtenstein", "Réunion", "North Macedonia", "Tajikistan", "Antigua & Barbuda", "Aruba", "Guinea-Bissau" 


```{r}

# Remove countries from google dataframe
google_countries <- google_countries %>% 
                     filter(!country %in% c("Hong Kong SAR China", "Mongolia", 
                                            "South Korea", "Taiwan", "Yemen", 
                                            "Liechtenstein", "Réunion", "North Macedonia", 
                                            "Tajikistan", "Antigua & Barbuda", "Aruba", 
                                            "Guinea-Bissau"))

```


# Detrend data
```{r}

 # Select the specific data and rename columns
google_countries <- google %>% select(country, ISO3, Date, 
                    `Nature`= 
                     parks_percent_change_from_baseline,
                    `Necessities` = 
                     grocery_and_pharmacy_percent_change_from_baseline,
                     Residential = 
                     residential_percent_change_from_baseline,
                    `Luxury` = 
                      retail_and_recreation_percent_change_from_baseline) 

# Remove countries from google dataframe
google_countries <- google_countries %>% 
                     filter(!country %in% c("Hong Kong SAR China", "Mongolia", 
                                            "South Korea", "Taiwan", "Yemen", 
                                            "Liechtenstein", "Réunion", "North Macedonia", 
                                            "Tajikistan", "Antigua & Barbuda", "Aruba", 
                                            "Guinea-Bissau"))

# Define month
google_countries$Month <- format(google_countries$Date,"%m")
google_countries$Month <- format(google_countries$Date,"%m")
google_countries$Month <- as.numeric(google_countries$Month)


# Run GAMM model through the month to remove seasonal trend
library(mgcv)
m.lux<-gamm(Luxury~ s(Month), random=list(country=~1), data=google_countries)


## Change luxury to numeric and country to factor
google_countries$Luxury <- as.numeric(google_countries$Luxury)
google_countries$country <- factor(google_countries$country)

# Explore model as GAM
m.lux2<-gam(Luxury~ s(Month) + s(country, bs="re"), method = "REML", data=google_countries)


## Add residuals to the google.countries dataframe
## There are some NAs in the data, so have to do a couple more steps than just
## adding the residuals as an additional column
google_countries2 <- google_countries %>% drop_na(Luxury)
google_countries2$res.Luxury<-residuals(m.lux2)
google_countries <- left_join(google_countries, google_countries2)


# Calculate the mean of Luxury and the model residuals for the first two weeks
# Then calculate the difference between the means 
recenter<- google_countries %>% group_by(country) %>%  select(Luxury, res.Luxury) %>% 
  slice(1:14) %>% summarise(mean.Lux = mean(Luxury),
                            mean.res = mean(res.Luxury),
                            diff = mean.res - mean.Lux)


# Recenter the residuals based on the difference between the means
google_countries$recenter.lux <- google_countries$res.Luxury - 4.301263


# Plot
ggplot(google_countries %>% filter(country == "Canada")) +
  theme_bw()+
  geom_point(aes(Date, Luxury))+
    geom_point(aes(Date,res.Luxury),colour = "red", alpha = 0.4)
   geom_point(aes(Date,recenter.lux),colour = "blue",  alpha = 0.4)





### TEST WITH CANADA
can <- read.csv("canada.csv")
can$Date <- as.POSIXct(can$date, format='%Y-%m-%d')
can$Month <- format(can$Date,"%m")
can$Month <- as.numeric(can$Month)

# Model
m1<-gam(Luxury~s(Month),data=can)

# Calculate residuals
can$res.Luxury<-residuals(m1)

# Caluclate the difference between Luxury and the model prediction
# ***This produces the same as the resudual code above***
pred.m1 <- predict.gam(m1, type="response")
can$pred <- pred.m1
can$res <- can$Luxury - can$pred

# Calculate the mean of Luxury and the model residuals for the first two weeks
# Then calculate the difference between the means 
can %>% select(Luxury, res.Luxury) %>% 
  slice(1:14) %>% summarise(mean.Lux = mean(Luxury),
                            mean.res = mean(res.Luxury),
                            diff = mean.res - mean.Lux)


# Recenter the residuals based on the difference between the means
can$recenter <- can$res.Luxury - 19.80977


# Plot
ggplot(can) +
  theme_bw()+
  geom_point(aes(Date, Luxury))+
   geom_point(aes(Date,res.Luxury),colour = "red", alpha = 0.4)
   geom_point(aes(Date,recenter),colour = "blue",  alpha = 0.4)


```



## Find the day people were home the most (peak lockdown)

```{r}

## Less that May 1st and the greatest at home to capture the first peak
## We select May because this is when most countries begin to ease the restrictions
## and May 1st was identified in a sensitivity test in Bates et al. (2021) Biological Conservation
home <- google_countries %>% group_by(country, ISO3) %>% 
            filter(Date <= "2020-05-01") %>%
            filter(Residential == max(Residential, na.rm = T)) %>%
            summarise(max.home = min(Date))


## Join
google_countries <- left_join(google_countries, home)


```

## Date countries cross zero

```{r}
library(tidyverse)

## Identify the day that a country's mobility crosses zero
## and calculate the number of days after lockdown that the 
## country's mobility crossed zero

# Nature (Parks and Beaches)
nature <- google_countries %>% group_by(country, ISO3) %>% 
            filter(Date > max.home & `Parks & Beaches` > 0) %>%
            summarise(home =  min(max.home), # the date people were home most
                      parks_zero = min(Date), # The first date people returned to nature
                      parks_diff = parks_zero - home) # calc difference 
            

# Necessities (Groceries and pharmacies)
necessities  <- google_countries %>% group_by(country, ISO3) %>% 
            filter(Date > max.home & `Grocery & Pharmacy` > 0) %>%
            summarise(home =  min(max.home), # the date people were home most
                      necessities_zero = min(Date), # The first date people returned to necessities
                      necessities_diff = necessities_zero - home) # calc difference 


# Luxuries (Retail & Recreation)
luxuries <- google_countries %>% group_by(country, ISO3) %>% 
            filter(Date > max.home & `Retail & Recreation` > 0) %>%
            summarise(home =  min(max.home), # the date people were home most
                      luxuries_zero = min(Date), # The first date people returned to luxuries
                      luxuries_diff = luxuries_zero - home) # calc difference 

# Join the dataframes
returns <- left_join(necessities, luxuries)
returns <- left_join(returns, nature)

# Global median for return dates
median(returns$necessities_diff, na.rm = T) # 48 days after peak lockdown
median(returns$parks_diff, na.rm = T) # 78 after peak lockdown
median(returns$luxuries_diff, na.rm = T) # 153 after peak lockdown

```


# Continental differences

```{r}

# Load packages
library(ggplot2);library(viridis); library(ggpubr);library(sf);library("rnaturalearth"); library("rnaturalearthdata"); library(tidyr)


# extract continent info
# These data will be used to match the world regions to the google data
world <- ne_countries(scale = "medium", returnclass = "sf")
world <- world %>% rename(ISO3 = iso_a3) 
region_un <- world$region_un
continent <- world$continent
subregion <- world$subregion
ISO3 <- world$ISO3
continent<- tibble(ISO3, region_un, continent, subregion)


# join the regional data to the google data
returns <- left_join(returns, continent, by = "ISO3")

# assign Tuvalu to Oceania
returns$region_un[is.na(returns$region_un)] <-"Oceania"


```


## Create World Map Plot

```{r}

#Get world map info
world_map <- map_data("world") %>%  
              group_by(region) %>% 
              summarise(long=mean(long),
                        lat=mean(lat)) %>%
              rename(country = region)

# Join the data
returns <- left_join(returns, world_map)


## Summarise the subregion/continental differences
regions <- returns %>% group_by(subregion) %>% 
            summarise(parks = median(naturediff, na.rm = T),
                      parks.min = min(naturediff, na.rm = T),
                      parks.max = max(naturediff, na.rm = T),
                      lux = median(luxdiff, na.rm = T),
                      lux.min = min(luxdiff, na.rm = T),
                      lux.max = max(luxdiff, na.rm = T),
                      nature = median(parks_diff, na.rm = T),
                      necessities = median(necessities_diff, na.rm = T),
                      luxuries = median(luxuries_diff, na.rm = T),
                      long=mean(long, na.rm = T),
                      lat=mean(lat, na.rm = T),
                      countries = n()) %>%
            arrange(parks)

# Save
write.csv(regions, "regions.csv")

## Colour countries that we have used
# Set colors
world <- mutate(world, fill = ifelse(ISO3 %in% returns$ISO3, 
                                     "Monitored", "Non-monitored"))


## Set the map theme
maptheme <-  theme_bw(base_size = 15)+
  theme(axis.title.y = element_blank(), 
        axis.title.x = element_blank(),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        legend.title = element_blank(),
        legend.position="none",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())


# World map with dots for locations
map.plot<- ggplot() +
    geom_sf(data = world, fill = "snow3", colour = "snow3", size = 0.2) +
     maptheme+
    geom_sf(data = world, aes(fill = fill), colour = "cornsilk", size = 0.01) +
    scale_fill_manual(values = c("cornsilk", "snow3"))+
    geom_point(data = regions, 
               aes(x = long, y = lat),
               shape=21)+
    coord_sf(xlim = c(-180, 220), ylim = c(-65, 120), expand = FALSE)



# Save the plot
ggsave("map plot.pdf", 
       width = 180, height = 150, unit = "mm")

```

## Bar plots of regional trends

```{r}

# Gather the data for plotting
map.regions <- regions %>% 
            select(subregion, countries, 
                   nature, luxuries, necessities, 
                   long, lat)  %>%
            gather(Mobility, Difference, nature:necessities)

# define for plotting
map.regions$mob <- "mob"

## Set plot theme
plot.theme <-  theme_void()+
  theme(legend.title = element_blank(),
        legend.position="none")


## Wrap the text into two lines when names are too long
library(stringr)
map.regions$subregion2 <-str_wrap(map.regions$subregion, 20)


library(viridis)

## plot the number of day after peak lockdown
bars <- ggplot(data = map.regions,
       aes(x = mob, y = Difference, fill = Mobility)) +
     scale_fill_viridis(discrete = TRUE, direction = -1)+
    #scale_fill_manual(values = c("#C15CCB","#00868B", "#FF6A00"))+
    geom_bar(stat= "identity",width = 0.3, position = position_dodge(0.35))+
    #geom_hline(yintercept = 0, size = 0.1)+
    facet_wrap(~subregion2, strip.position = "bottom")+
    plot.theme


# Save the plot
ggsave("bars plot.pdf", 
       width = 150, height = 90, unit = "mm")
```



## Global plot for time at nature, necessities, luxuries

Here we calculate and plot the global median change of time at nature, necessities, luxuries after lockdown.


```{r}
library(dplyr); library(tidyr)

## Calculate global median movement at nature, necessities, luxuries
google_median <- google_countries %>% group_by(Date) %>% 
      summarise(`Parks & Beaches`= median(`Parks & Beaches`, na.rm = TRUE),
                `Grocery & Pharmacy` = median(`Grocery & Pharmacy`, na.rm = TRUE),
                `Retail & Recreation` = median(`Retail & Recreation`, na.rm = TRUE))


## Rearrange for plotting
google_median <- google_median %>% gather(type, change, `Parks & Beaches`:`Retail & Recreation`)

## Define plot theme
global_plottheme <- theme_bw(base_size = 15)+ # set the background theme       
  theme(panel.grid.major = element_blank(), # remove the major lines
        panel.grid.minor = element_blank(), # remove the minor lines
        axis.title.x = element_blank(),
        strip.text.x = element_text(size = 15, color = "white"), 
        strip.background = element_rect(fill="black"),
        legend.title = element_blank(),
        legend.position="none")


## Create global plot
timeplot<- ggplot(google_median, aes(Date, change)) +
  geom_line( aes(colour = type))+
  geom_hline(yintercept = 0, linetype = "dashed")+
  labs(y = "Change in length \n of visit (%)")+
  #scale_colour_manual(values = c( "#C15CCB","#00868B", "#FF6A00"))+
  scale_colour_viridis(discrete = TRUE)+
  global_plottheme


# Save the plot
ggsave("time plot.pdf", 
       width = 180, height = 75, unit = "mm")

```


## Combine the plots 

We manually place the bar plots onto the map.

```{r}

library(patchwork)

map.plot / timeplot + plot_layout(height=c(2,1))

# Install ggview
remotes::install_github("idmn/ggview")
library(ggview)
ggview(units = "mm", width = 150, height = 90)

```

